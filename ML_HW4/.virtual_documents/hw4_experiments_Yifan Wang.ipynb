import numpy as np
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm


def train(lr = 0.1, steps = 15):
    model = torch.nn.Linear(4, 1, bias=False)

    # fix the initialization for reproducing results, do not change. In practice, we use random initialization
    model.weight.data = torch.FloatTensor([[ 2.0871, -1.3213,  0.7339, -1.0858]])

    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    loss_func = torch.nn.functional.mse_loss

    train_loss_list = []

    model.train()

    for _ in tqdm(range(steps)):
        pred = model(X_train)

        # reduction='sum' is for consistent computation for linear regression. In practice, we often use reduction='mean'
        loss = 0.5 * loss_func(pred, y_train, reduction='sum')

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss_list.append(loss.detach().item())

    return train_loss_list











# load the data
X_train = np.load('X_train_0.npy')
y_train = np.load('y_train_0.npy')

print(X_train)

X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train).unsqueeze(1)

X_transpose_X = np.dot(X_train.T, X_train)
print(X_transpose_X)
eigenvalues =  np.linalg.eigvals(X_transpose_X)
largest_eigenvalue = np.max(eigenvalues)
smallest_eigenvalue = np.min(eigenvalues)
print(f'Largest:{largest_eigenvalue}， Smallest:{smallest_eigenvalue}')


loss_vs_lr = dict()

lr_list = np.linspace(0.1, 2.0, 20)

for lr in lr_list:
    loss_vs_lr[lr] = train(lr=lr, steps=15)[-1]
    print(loss_vs_lr[lr])

# log_losses = np.log([loss_vs_lr[lr] for lr in lr_list])
# print (log_losses)


plt.figure(figsize=(10,6))
for lr in lr_list:
    plt.plot(lr_list, list(loss_vs_lr.values()), label=f"lr = {lr}")

plt.yscale('log')
plt.title("Training loss")
plt.legend(fontsize=6)

# plt.plot(lr_list, log_losses, marker='o', linestyle='-', color='b')
# plt.xlabel('Learning Rate (η)')
# plt.ylabel('Log of Training Loss')
# plt.title('Training Loss after 15 Steps vs Learning Rate')
# plt.legend([f"lr = {lr:.2f}" for lr in lr_list])
# plt.show()











# load the data
X_train = np.load('X_train_1.npy')
y_train = np.load('y_train_1.npy')

X_train = torch.FloatTensor(X_train)
y_train = torch.FloatTensor(y_train).unsqueeze(1)

X_transpose_X = np.dot(X_train.T, X_train)
eigenvalues =  np.linalg.eigvals(X_transpose_X)
largest_eigenvalue = np.max(eigenvalues)
smallest_eigenvalue = np.min(eigenvalues)
print(f'Largest:{largest_eigenvalue}， Smallest:{smallest_eigenvalue}')


loss_vs_lr = dict()

lr_list = np.linspace(0.1, 2.0, 20)

for lr in lr_list:
    loss_vs_lr[lr] = train(lr=lr, steps=600)[-1]
    print(loss_vs_lr[lr])


plt.figure(figsize=(10,6))
for lr in lr_list:
    plt.plot(lr_list, list(loss_vs_lr.values()), label=f"lr = {lr}")

plt.yscale('log')
plt.title("Training loss")






